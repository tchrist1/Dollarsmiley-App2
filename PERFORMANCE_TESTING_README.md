# Performance Testing - Home Screen & Filters

## üéØ Quick Navigation

| I want to... | Read this file... |
|--------------|-------------------|
| **Get started quickly** | `PERFORMANCE_TEST_QUICK_START.txt` |
| **Run tests step-by-step** | `MANUAL_PERFORMANCE_TEST_GUIDE.md` |
| **Record my results** | `PERFORMANCE_TEST_RESULTS_TEMPLATE.md` |
| **Understand the implementation** | `PERFORMANCE_TEST_IMPLEMENTATION_SUMMARY.md` |
| **See the big picture** | `PERFORMANCE_TEST_FINAL_SUMMARY.md` (or keep reading) |

---

## üìã What This Is

An automated, repeatable performance testing system for measuring Home screen and Filters functionality.

**What it does:**
- ‚úÖ Measures timing, network calls, renders, and more
- ‚úÖ Identifies performance bottlenecks
- ‚úÖ Provides baseline metrics for optimization decisions

**What it does NOT do:**
- ‚ùå Modify any business logic
- ‚ùå Change any UI behavior
- ‚ùå Optimize performance (measurement only)
- ‚ùå Impact production builds (DEV-only)

---

## üöÄ Getting Started (2 Minutes)

### Step 1: Start the App

```bash
npm run dev
```

### Step 2: Open Console

- **Browser**: Press F12 ‚Üí Console tab
- **React Native**: Open debugger or Expo DevTools

### Step 3: Look for [PERF] Logs

All performance logs are prefixed with `[PERF]`:

```
[PERF] FILTER_OPEN_TAP { timestamp: "1234.56" }
[PERF] FILTER_OPEN_VISIBLE { timestamp: "1484.56" }
```

**Calculate performance**: `1484.56 - 1234.56 = 250ms`

That's it! You're measuring performance.

---

## üìñ Complete Testing Guide

For full step-by-step instructions, see:

üëâ **[MANUAL_PERFORMANCE_TEST_GUIDE.md](./MANUAL_PERFORMANCE_TEST_GUIDE.md)**

This guide covers:
- How to perform each test
- What console output to expect
- How to calculate metrics
- How to interpret results

---

## üìä What Gets Tested

| Test | What It Measures | Target |
|------|------------------|--------|
| **Open Filters** | Modal opening time | <200ms |
| **Close Filters** | Modal closing time | <200ms |
| **Apply Job Filter** | Filter + data fetch | <500ms |
| **Clear All (Job)** | Reset + refresh | <200ms |
| **Apply Service Filter** | Filter + data fetch | <500ms |
| **Clear All (Service)** | Reset + refresh | <200ms |
| **First Home Load** | Initial page load | <1000ms |

Each test runs **3 times** to collect average and P95 (95th percentile) metrics.

---

## üìà Metrics Collected

For each test, you get:

| Metric | What It Means | Good | Needs Work |
|--------|---------------|------|------------|
| **Average Time** | Mean execution time | <200ms UI | >500ms UI |
| **P95 Time** | Worst-case timing | <300ms UI | >750ms UI |
| **JS Blocks** | Main thread blocks | 0-1 | >3 |
| **Frame Drops** | Animation jank | 0-2 | >5 |
| **Network Calls** | API requests | <3 | >5 |
| **Re-renders** | Component renders | 1-3 | >7 |

---

## üìù Recording Results

Use this template to record your findings:

üëâ **[PERFORMANCE_TEST_RESULTS_TEMPLATE.md](./PERFORMANCE_TEST_RESULTS_TEMPLATE.md)**

---

## üîç Understanding the Implementation

### How It Works

1. **Instrumentation**: DEV-only code logs performance events
2. **Console Logs**: All events appear as `[PERF]` logs
3. **Manual Testing**: You perform actions and record timings
4. **Analysis**: Calculate metrics and identify bottlenecks

### Code Changes

Only 2 files were modified (47 lines total, all DEV-only):

```typescript
// components/FilterModal.tsx
if (__DEV__) {
  logPerfEvent('FILTER_OPEN_TAP');
}

// app/(tabs)/index.tsx
if (__DEV__) {
  logPerfEvent('HOME_FIRST_RENDER');
}
```

All instrumentation is behind `__DEV__` guards:
- ‚úÖ Zero production impact
- ‚úÖ Zero bundle size increase
- ‚úÖ Zero runtime overhead in production

---

## üéì Example Test Session

Here's what a complete test looks like:

```bash
# 1. Start app
npm run dev

# 2. Open console and watch for logs

# 3. Tap "Filters" button
[PERF] FILTER_OPEN_TAP { timestamp: "1234.56" }
[PERF] FILTER_MODAL_OPENING { filtersCount: 0 }
[PERF] FILTER_OPEN_VISIBLE { timestamp: "1484.56" }

# 4. Calculate: 1484.56 - 1234.56 = 250ms ‚úì

# 5. Repeat 2 more times, record all 3 values

# 6. Calculate average and P95
Average: (250 + 255 + 249) / 3 = 251.33ms
P95: 255ms (2nd highest)

# 7. Record in template
```

---

## üõ†Ô∏è Troubleshooting

### No [PERF] Logs Appearing?

**Check:**
- Running dev mode (not production build)
- Console is visible
- Not filtering console output

**Fix:**
```bash
# Stop app
# Clear cache
# Restart: npm run dev
```

### Results Inconsistent?

**Causes:**
- Background apps
- Network conditions
- Server load

**Solutions:**
- Close other apps
- Use consistent network
- Run additional iterations

### Can't Calculate Metrics?

**Use this formula:**
```
Duration = End Timestamp - Start Timestamp

Example:
FILTER_OPEN_TAP at 1234.56
FILTER_OPEN_VISIBLE at 1484.56
Duration = 1484.56 - 1234.56 = 250ms
```

---

## üìö Documentation Index

| Document | Purpose | Read When... |
|----------|---------|--------------|
| `PERFORMANCE_TESTING_README.md` | Overview (this file) | Starting out |
| `PERFORMANCE_TEST_QUICK_START.txt` | One-page reference | Need quick lookup |
| `MANUAL_PERFORMANCE_TEST_GUIDE.md` | Step-by-step guide | Running tests |
| `PERFORMANCE_TEST_RESULTS_TEMPLATE.md` | Results recording | Recording data |
| `PERFORMANCE_TEST_IMPLEMENTATION_SUMMARY.md` | Technical details | Understanding code |
| `PERFORMANCE_TEST_FINAL_SUMMARY.md` | Executive summary | Big picture view |

---

## üéØ Success Checklist

You're successful when you can:

- [ ] See `[PERF]` logs in console
- [ ] Measure time between events
- [ ] Run each test 3 times consistently
- [ ] Calculate average and P95 metrics
- [ ] Fill out the results template
- [ ] Identify top 3 bottlenecks

---

## üö¶ Next Steps

### Now: Run Tests

1. **Read**: `MANUAL_PERFORMANCE_TEST_GUIDE.md`
2. **Start**: `npm run dev`
3. **Test**: Perform each test 3 times
4. **Record**: Fill out `PERFORMANCE_TEST_RESULTS_TEMPLATE.md`

**Time Required**: ~30 minutes

### Later: Analyze Results

1. Review metrics against benchmarks
2. Identify top 3 bottlenecks
3. Document findings
4. Create optimization plan (separate task)

---

## ‚ö†Ô∏è Important Notes

### This is Measurement-Only

**DO NOT optimize based on findings without:**
1. Documenting the optimization plan
2. Getting approval for changes
3. Creating a separate task
4. Re-measuring after changes

### All Changes Are DEV-Only

No production impact:
- Instrumentation only runs in dev mode
- Zero bundle size increase in production
- Zero runtime overhead in production
- All changes behind `__DEV__` guards

### No Behavior Changes

App works identically:
- Same business logic
- Same UI behavior
- Same network calls
- Same state management

Only difference: Performance logs in DEV mode

---

## üí° Tips for Accurate Testing

1. **Consistent Environment**: Same device, network, time of day
2. **Clean State**: Close background apps, clear cache
3. **Multiple Runs**: 3 runs minimum, more if variance >10%
4. **Document Everything**: Screenshots, notes, observations
5. **Watch for Patterns**: Consistent vs. sporadic slowness

---

## üìû Need Help?

**Quick Reference**: `PERFORMANCE_TEST_QUICK_START.txt`

**Step-by-Step Guide**: `MANUAL_PERFORMANCE_TEST_GUIDE.md`

**Technical Details**: `PERFORMANCE_TEST_IMPLEMENTATION_SUMMARY.md`

**Big Picture**: `PERFORMANCE_TEST_FINAL_SUMMARY.md`

---

## ‚úÖ Status

- **Implementation**: ‚úÖ Complete
- **Ready for Testing**: ‚úÖ Yes
- **Production Impact**: ‚úÖ None
- **Behavior Changes**: ‚úÖ None
- **Documentation**: ‚úÖ Complete

---

## üéâ You're Ready!

The performance testing system is fully implemented and ready to use.

**Start here**: `MANUAL_PERFORMANCE_TEST_GUIDE.md`

**Quick reference**: `PERFORMANCE_TEST_QUICK_START.txt`

**Have questions?** Check the relevant document from the index above.

---

*Performance testing infrastructure successfully implemented - Ready for measurement*
